{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name classification with Keras\n",
    "\n",
    "In this example we will try to reproduce the results presented in [this paper](https://www.ijcai.org/proceedings/2017/0289.pdf). Here the authors classify name nationalities using LSTMs with a clever trick that we will describe later. \n",
    "\n",
    "The dataset that we will use in this example can be downloaded from [here](https://www.dropbox.com/s/vx88k39dja9zcxj/data.tar.gz?dl=0)\n",
    "\n",
    "Place the two files into a directory called `data`. In addition, create two directories called `data_processed` and `models` where we will store the results of processing the raw data and the `keras` models. \n",
    "\n",
    "The directory structure should be:\n",
    "\n",
    "```\n",
    ".\n",
    "├── data\n",
    "│   ├── country2ethnicity.txt\n",
    "│   └── countryResult.txt\n",
    "├── data_processed\n",
    "├── models\n",
    "└── name_classification_rnn.ipynb\n",
    "```\n",
    "\n",
    "Let's have a look to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "from nltk import ngrams\n",
    "from gensim.models import Word2Vec\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Flatten, LSTM, Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.metrics import top_k_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file = \"./data/countryResult.txt\"\n",
    "data_dir = \"./data_processed\"\n",
    "dataset = open(raw_data_file).read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 31595 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Belarus\\tBeijing 2008\\tsilver\\t20.28\\tAthletics\\tNatallia MIKHNEVICH/',\n",
       " 'Belarus\\tVancouver 2010\\tsilver\\t48:32.0\\tBiathlon\\tSergey NOVIKOV/',\n",
       " 'Belarus\\tBeijing 2008\\tsilver\\t8551\\tAthletics\\tAndrei KRAUCHANKA/',\n",
       " 'Belarus\\tVancouver 2010\\tgold\\tFINAL\\tFreestyle Skiing\\tAlexei GRISHIN/',\n",
       " 'Belarus\\tBeijing 2008\\tbronze\\t81.51\\tAthletics\\tIvan TSIKHAN/']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the data and build more functional objects/dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_chars = [':', '©', '¶']\n",
    "def clean_names(name):\n",
    "    name_text = BeautifulSoup(name).get_text()\n",
    "    name_text = re.sub(\"[^a-zA-Z\\'.']\",\" \", name_text)\n",
    "    name_text = re.sub(\" +\",\" \",name_text)\n",
    "    name_text = name_text.strip()\n",
    "    clean_name = name_text.title()\n",
    "    return clean_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31595/31595 [01:29<00:00, 351.97it/s]\n"
     ]
    }
   ],
   "source": [
    "name2country = dict()\n",
    "name2year = dict()\n",
    "for line in tqdm(dataset):\n",
    "    try:\n",
    "        country, olympic_year, medal, record, sports, names_raw = line.split('\\t')\n",
    "        country = country.replace(',', ' ')\n",
    "        country = country.strip()\n",
    "    except ValueError as e:\n",
    "        pass\n",
    "    # In the olympics one has teams (i.e more than one individual per row)\n",
    "    if len(names_raw.split('/')) >= 2:\n",
    "        names = names_raw.split('/')\n",
    "        names = [n for n in names if n!=\"\"]\n",
    "        for name in names:\n",
    "            c_name = clean_names(name)\n",
    "            if c_name in name2country:\n",
    "                # and some athlete change countries. We keep the most recent nationality\n",
    "                if country != name2country[c_name]:\n",
    "                    previous_year = int(name2year[c_name].split(' ')[-1])\n",
    "                    current_year  = int(olympic_year.split(' ')[-1])\n",
    "                    if  previous_year <= current_year:\n",
    "                        continue\n",
    "                    else:\n",
    "                        pass\n",
    "            name2country[c_name] = country\n",
    "            name2year[c_name] = olympic_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the cleaning we lose 14k observations approximately. This is partially due to the fact that the cleaning above is fairly \"rough\". I will leave to you to carry out a better cleaning of the data so we keep more observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17715"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name2country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natallia Mikhnevich', 'Belarus'),\n",
       " ('Sergey Novikov', 'Belarus'),\n",
       " ('Andrei Krauchanka', 'Belarus'),\n",
       " ('Alexei Grishin', 'Belarus'),\n",
       " ('Ivan Tsikhan', 'Belarus')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(list(name2country.keys())[:5], list(name2country.values())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Algeria', 0),\n",
       " ('Argentina', 1),\n",
       " ('Armenia', 2),\n",
       " ('Australasia (1908-1912)', 3),\n",
       " ('Australia', 4)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country2idx = dict([(cntr,i) for i,cntr in enumerate(sorted(set(name2country.values())))])\n",
    "list(zip(list(country2idx.keys())[:5], list(country2idx.values())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the resulst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(name2country, open(os.path.join(data_dir, 'name2country.p'), 'wb'))\n",
    "pickle.dump(country2idx, open(os.path.join(data_dir,'country2idx.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a helper function to get the n-grams given a name. We will see later what these are used for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(corpus, n):\n",
    "    \"\"\"\n",
    "    Helper function that returns n-grams given a corpus of words/tokens\n",
    "    (names in this case)\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    corpus: list\n",
    "        list with names\n",
    "    n: int\n",
    "        ngram order\n",
    "    Returns:\n",
    "    -------\n",
    "    list(ngrams): list\n",
    "        list of ngrams        \n",
    "    \"\"\"\n",
    "    n_grams = set()\n",
    "    for strg in corpus:\n",
    "        ngram_gen = ngrams(strg,n)\n",
    "        for n_gram in ngram_gen:\n",
    "            n_grams.add(\"\".join(n_gram))\n",
    "    return list(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vi', 'av', 'ja', 'er', 'ie']\n",
      "['avi', 'ier', 'jav', 'vie']\n"
     ]
    }
   ],
   "source": [
    "print(get_ngram(['javier'], 2))\n",
    "print(get_ngram(['javier'], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = name2country.keys()\n",
    "unigrams = sorted(list(set(\" \".join(all_names))))\n",
    "bigrams  = sorted(get_ngram(all_names, 2))\n",
    "trigrams = sorted(get_ngram(all_names, 3))\n",
    "unigram2idx = dict([(ng, i) for i,ng in enumerate(unigrams)])\n",
    "bigram2idx  = dict([(ng, i) for i,ng in enumerate(bigrams)])\n",
    "trigram2idx = dict([(ng, i) for i,ng in enumerate(trigrams)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the resulst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(unigram2idx, open(os.path.join(data_dir,'unigram2idx.p'), 'wb'))\n",
    "pickle.dump(bigram2idx, open(os.path.join(data_dir,'bigram2idx.p'), 'wb'))\n",
    "pickle.dump(trigram2idx, open(os.path.join(data_dir,'trigram2idx.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the aim of preserving order, let's move from dictionaries to tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..... Daumain',\n",
       " 'A Lam Shin',\n",
       " 'A. Albert',\n",
       " 'A. B Gli',\n",
       " 'A. B. Zumelzu',\n",
       " 'A. Faehlmann',\n",
       " 'A. Fasani',\n",
       " 'A. Fauquet Lemaitre',\n",
       " 'A. Ferraris',\n",
       " 'A. Gilpin']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to preserve order, so dictionaries are not good...\n",
    "tmp = list(name2country.items())\n",
    "tmp = sorted(tmp, key=lambda tmp: tmp[0])\n",
    "all_names, all_countries = [], []\n",
    "for n, c in tmp:\n",
    "    all_names.append(n)\n",
    "    all_countries.append(c)\n",
    "all_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and build our corpus of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['..', '..', '..', '..', '. ', ' D', 'Da', 'au', 'um', 'ma', 'ai', 'in'],\n",
       " ['A ', ' L', 'La', 'am', 'm ', ' S', 'Sh', 'hi', 'in'],\n",
       " ['A.', '. ', ' A', 'Al', 'lb', 'be', 'er', 'rt'],\n",
       " ['A.', '. ', ' B', 'B ', ' G', 'Gl', 'li'],\n",
       " ['A.', '. ', ' B', 'B.', '. ', ' Z', 'Zu', 'um', 'me', 'el', 'lz', 'zu']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build corpus of ngrams with n=1,2,3\n",
    "unig_corpus = [list((''.join(ng) for ng in ngrams(name, 1)))\n",
    "               for name in all_names]\n",
    "bigr_corpus = [list((''.join(ng) for ng in ngrams(name, 2)))\n",
    "               for name in all_names]\n",
    "trig_corpus = [list((''.join(ng) for ng in ngrams(name, 3)))\n",
    "               for name in all_names]\n",
    "\n",
    "# let's have a look to the bigram corpus\n",
    "bigr_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to numerically encode the sequences using the n-grams2idx dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[45, 45, 45, 45, 44, 4, 127, 533, 1019, 820, 521, 728],\n",
       " [66, 12, 269, 525, 819, 19, 400, 699, 728],\n",
       " [67, 44, 1, 79, 795, 544, 632, 947],\n",
       " [67, 44, 2, 94, 7, 188, 802],\n",
       " [67, 44, 2, 95, 44, 26, 507, 1019, 824, 626, 818, 1130]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unig_seq = [list(unigram2idx[gram] for gram in name)\n",
    "            for name in unig_corpus]\n",
    "bigr_seq = [list(bigram2idx[gram] for gram in name)\n",
    "            for name in bigr_corpus]\n",
    "trig_seq = [list(trigram2idx[gram] for gram in name)\n",
    "            for name in trig_corpus]\n",
    "bigr_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passing the data to the network, we need to ensure that all sequences have the same length. Based on the length of the names I decided to use`MAX_SEQUENCE_LENGTH = 30`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17715, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,   45,   45,   45,   45,\n",
       "         44,    4,  127,  533, 1019,  820,  521,  728], dtype=int32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 30\n",
    "unig_X = np.vstack(pad_sequences(unig_seq, MAX_SEQUENCE_LENGTH))\n",
    "bigr_X = np.vstack(pad_sequences(bigr_seq, MAX_SEQUENCE_LENGTH))\n",
    "trig_X = np.vstack(pad_sequences(trig_seq, MAX_SEQUENCE_LENGTH))\n",
    "print(bigr_X.shape)\n",
    "bigr_X[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's build our target for the classification problem: nationality/ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>ARA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>SPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>EEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australasia (1908-1912)</td>\n",
       "      <td>ELSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>ENG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   country ethnicity\n",
       "0                  Algeria       ARA\n",
       "1                Argentina       SPA\n",
       "2                  Armenia       EEU\n",
       "3  Australasia (1908-1912)      ELSE\n",
       "4                Australia       ENG"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country2ethnicity = pd.read_csv('data/country2ethnicity.txt', header=None, names=['country', 'ethnicity'])\n",
    "country2ethnicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ARA': 0, 'CEA': 1, 'CHI': 2, 'EEU': 3, 'ELSE': 4, 'ENG': 5, 'FRA': 6, 'GER': 7, 'GRE': 8, 'IND': 9, 'ITA': 10, 'JAP': 11, 'KOR': 12, 'NEU': 13, 'NHL': 14, 'POR': 15, 'RUS': 16, 'SPA': 17}\n"
     ]
    }
   ],
   "source": [
    "ethnicity2idx = sorted(country2ethnicity.ethnicity.unique())\n",
    "ethnicity2idx = dict([(e, i) for i, e in enumerate(ethnicity2idx)])\n",
    "pickle.dump(ethnicity2idx, open('data_processed/ethnicity2idx.p', 'wb'))\n",
    "print(ethnicity2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compiling the model, the loss funciton we will use is `categorical_crossentropy`. Therefore, we need to one-hot encode categories. `Keras` makes our life easy as this can be done in a liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 12,  6,  7, 17,  3,  6,  4, 10,  4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country2ethnicity = country2ethnicity.replace({'ethnicity': ethnicity2idx})\n",
    "country2ethnicity = pd.Series(\n",
    "    country2ethnicity.ethnicity.values,\n",
    "    country2ethnicity.country.values\n",
    "    ).to_dict()\n",
    "Y = np.array([country2ethnicity[c] for c in all_countries])\n",
    "Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = to_categorical(Y)\n",
    "Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as with any other ML problem, train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "unig_X_tr, unig_X_te = train_test_split(\n",
    "    unig_X, test_size=0.3, random_state=1981)\n",
    "bigr_X_tr, bigr_X_te = train_test_split(\n",
    "    bigr_X, test_size=0.3, random_state=1981)\n",
    "trig_X_tr, trig_X_te = train_test_split(\n",
    "    trig_X, test_size=0.3, random_state=1981)\n",
    "Y_tr, Y_te = train_test_split(Y, test_size=0.3, random_state=1981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12400, 30) (5315, 30)\n",
      "(12400, 30) (5315, 30)\n",
      "(12400, 30) (5315, 30)\n"
     ]
    }
   ],
   "source": [
    "print(unig_X_tr.shape,unig_X_te.shape)\n",
    "print(bigr_X_tr.shape,bigr_X_te.shape)\n",
    "print(trig_X_tr.shape,trig_X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The following figure illustrates the model implemented by [Lee et al](https://www.ijcai.org/proceedings/2017/0289.pdf)\n",
    "\n",
    "<img src=\"images/architecture.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "The clever trick that the authors used consists in initialising the n-grams using the [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) algorithm. As a result, the n-grams will be initialise based on their context (i.e. other n-grams surrounding them), which might speed up convergence. \n",
    "\n",
    "To implement this initialization we will use the `gensim` package in `python`, which comes with a handy `Word2Vec` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define an initializer\n",
    "def initializer(sequences, ngram2idx, emb_dim):\n",
    "    \"\"\"\n",
    "    Function to initialize the weigths of the n-grams\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    sequences: list\n",
    "        list of numerically encoded n-gram sequences\n",
    "    ngram2idx: dict\n",
    "        dictionary with {'ngram': idx}\n",
    "    emb_dim: int\n",
    "        dimension of the embeddings to be learned\n",
    "    \"\"\"\n",
    "    # word2vec does not care about the nature of the token. Here we will pass indexes\n",
    "    sequences = [list((str(idx) for idx in gram)) for gram in sequences]\n",
    "\n",
    "    # word2vec model\n",
    "    model = Word2Vec(\n",
    "        sequences, \n",
    "        size=emb_dim, \n",
    "        window=5,      # context window\n",
    "        min_count=0, \n",
    "        iter=10)\n",
    "    \n",
    "    # matrix of zeros to be initialised with the embeddings\n",
    "    init = np.zeros((len(ngram2idx), emb_dim), dtype=np.float32)\n",
    "    for ngram, idx in ngram2idx.items():\n",
    "        init[idx] = model[str(idx)]\n",
    "    return init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 50)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unig_emb_init = initializer(unig_seq, unigram2idx, 50)\n",
    "bigr_emb_init = initializer(bigr_seq, bigram2idx,  100)\n",
    "trig_emb_init = initializer(trig_seq, trigram2idx, 150)\n",
    "unig_emb_init.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's define the model (all this should be wrapped up in a function. I will leave to you that exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 30, 50)       2750        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 30, 100)      113500      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 30, 150)      1617300     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 128)          91648       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 128)          117248      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 128)          142848      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           lstm_4[0][0]                     \n",
      "                                                                 lstm_5[0][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 18)           6930        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,092,224\n",
      "Trainable params: 2,092,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Input Layers\n",
    "unig_inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "bigr_inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "trig_inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "# Embeddings layers\n",
    "unig_emb_layer = Embedding(len(unigram2idx),\n",
    "                           50,\n",
    "                           weights=[unig_emb_init],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=True)\n",
    "bigr_emb_layer = Embedding(len(bigram2idx),\n",
    "                           100,\n",
    "                           weights=[bigr_emb_init],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=True)\n",
    "trig_emb_layer = Embedding(len(trigram2idx),\n",
    "                           150,\n",
    "                           weights=[trig_emb_init],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=True)\n",
    "\n",
    "# unigrams network\n",
    "unig_emb = unig_emb_layer(unig_inp)\n",
    "unig_lstm = LSTM(128, dropout=0.3, recurrent_dropout=0.3)(unig_emb)\n",
    "\n",
    "# bigrams network\n",
    "bigr_emb = bigr_emb_layer(bigr_inp)\n",
    "bigr_lstm = LSTM(128, dropout=0.3, recurrent_dropout=0.3)(bigr_emb)\n",
    "\n",
    "# trigrams network\n",
    "trig_emb = trig_emb_layer(trig_inp)\n",
    "trig_lstm = LSTM(128, dropout=0.3, recurrent_dropout=0.3)(trig_emb)\n",
    "\n",
    "# concatenate the output\n",
    "allgrams = concatenate([unig_lstm, bigr_lstm, trig_lstm])\n",
    "\n",
    "# final FC layer\n",
    "preds = Dense(len(ethnicity2idx), activation='softmax')(allgrams)\n",
    "\n",
    "model = Model([unig_inp, bigr_inp, trig_inp], preds)\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12400/12400 [==============================] - 18s 1ms/step - loss: 1.7676 - top_k_mod: 0.7094\n",
      "Epoch 2/10\n",
      "12400/12400 [==============================] - 14s 1ms/step - loss: 1.2865 - top_k_mod: 0.8322\n",
      "Epoch 3/10\n",
      "12400/12400 [==============================] - 15s 1ms/step - loss: 1.1224 - top_k_mod: 0.8697\n",
      "Epoch 4/10\n",
      "12400/12400 [==============================] - 15s 1ms/step - loss: 1.0026 - top_k_mod: 0.8948\n",
      "Epoch 5/10\n",
      "12400/12400 [==============================] - 15s 1ms/step - loss: 0.9099 - top_k_mod: 0.9069\n",
      "Epoch 6/10\n",
      "12400/12400 [==============================] - 14s 1ms/step - loss: 0.8089 - top_k_mod: 0.9267\n",
      "Epoch 7/10\n",
      "12400/12400 [==============================] - 14s 1ms/step - loss: 0.7157 - top_k_mod: 0.9418\n",
      "Epoch 8/10\n",
      "12400/12400 [==============================] - 15s 1ms/step - loss: 0.6226 - top_k_mod: 0.9556\n",
      "Epoch 9/10\n",
      "12400/12400 [==============================] - 14s 1ms/step - loss: 0.5446 - top_k_mod: 0.9636\n",
      "Epoch 10/10\n",
      "12400/12400 [==============================] - 15s 1ms/step - loss: 0.4689 - top_k_mod: 0.9738\n",
      "5315/5315 [==============================] - 3s 477us/step\n",
      "0.9064910630291627\n"
     ]
    }
   ],
   "source": [
    "def top_k_mod(y_true, y_pred, k=3):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=[top_k_mod])\n",
    "model.fit([unig_X_tr, bigr_X_tr, trig_X_tr], Y_tr, batch_size=128, epochs=10)\n",
    "_, top_k_acc = model.evaluate([unig_X_te, bigr_X_te, trig_X_te], Y_te)\n",
    "print(top_k_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/ngrams_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are overfitting and I am sure one could get better accuracy metrics. \n",
    "\n",
    "Let's see how to predict nationality given a certain name. Again this should be wrapped up in a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPA', 'POR', 'ITA']\n",
      "[0.41077068, 0.25426504, 0.113881]\n"
     ]
    }
   ],
   "source": [
    "idx2ethnicity = {v:k for k,v in ethnicity2idx.items()}\n",
    "\n",
    "name = \"Armando\"\n",
    "unigrams = [n[0] for n in ngrams(name,1)]\n",
    "bigrams  = [''.join(n) for n in ngrams(name,2)]\n",
    "trigrams = [''.join(n) for n in ngrams(name,3)]\n",
    "\n",
    "unig_inp = pad_sequences([[unigram2idx[n] for n in unigrams]],30)\n",
    "bigr_inp = pad_sequences([[bigram2idx[n] for n in bigrams]],30)\n",
    "trig_inp = pad_sequences([[trigram2idx[n] for n in trigrams]],30)\n",
    "\n",
    "preds = model.predict([unig_inp,bigr_inp,trig_inp])[0]\n",
    "scores = [preds[i] for i in np.argsort(-preds)[:3]]\n",
    "top3 = np.argsort(-preds)[:3]\n",
    "\n",
    "regions = []\n",
    "for idx in top3:\n",
    "    regions.append(idx2ethnicity[idx])\n",
    "\n",
    "print(regions)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
