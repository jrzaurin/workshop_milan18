{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN: Sentiment Analysis\n",
    "\n",
    "Get the data from here http://mng.bz/0tIo \n",
    "\n",
    "place the file in the `datasets` directory and unzip it \n",
    "\n",
    "Let's prepare the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = 'datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "When working with text we need to tokenize the data. In addition, the sequences we will pass to the RNNs need to be of the same length. To that aim we cut them at a certain length and if they are not long enough, we pad them with 0s at the beginning or the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  777   16   28    4    1  115 2278 6887   11   19\n",
      " 1025    5   27    5   42 2425 1861  128 2270    5    3 6985  308    7\n",
      "    7 3383 2373    1   19   36  463 3169    2  222    3 1016  174   20\n",
      "   49  808]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# maximum review length\n",
    "maxlen = 100\n",
    "validation_samples = 10000 \n",
    "# maximum words in our vocabulary\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's reconstruct a sequence to check that we are doing the right job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with one of the best shakespeare sources this film manages to be to it's source whilst still appealing to a wider audience br br branagh steals the film from under nose and there's a talented cast on good form\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for el in data[0]:\n",
    "    w = [k for k,v in word_index.items() if v==el]\n",
    "    if w:\n",
    "        words.append(w[0])\n",
    "\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as with any other ML problem, let's split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings\n",
    "\n",
    "I hope we are familiar with word vectors, and the concept of embeddings...\n",
    "\n",
    "Go [here](https://nlp.stanford.edu/projects/glove/) and download the Glove vectors. Place the file in your `dataset` directory and unzip it\n",
    "\n",
    "Word Vectors are numerical representations of words that encode semantic meaning. Such encoding is attained by examining the context of a given word. In other words, words surrounding by similar words will have similar encodings/embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'datasets/glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Embedding Matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1: non trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(embedding_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# or \n",
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 67s 4ms/step - loss: 0.6100 - acc: 0.6574 - val_loss: 0.4885 - val_acc: 0.7680\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 67s 4ms/step - loss: 0.4781 - acc: 0.7719 - val_loss: 0.4185 - val_acc: 0.8057\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 66s 4ms/step - loss: 0.4273 - acc: 0.8042 - val_loss: 0.4019 - val_acc: 0.8163\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 66s 4ms/step - loss: 0.3960 - acc: 0.8198 - val_loss: 0.3797 - val_acc: 0.8273\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 66s 4ms/step - loss: 0.3787 - acc: 0.8263 - val_loss: 0.3653 - val_acc: 0.8372\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2: trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words,\n",
    "                    embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "model2.add(LSTM(embedding_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 81s 5ms/step - loss: 0.5431 - acc: 0.7134 - val_loss: 0.3709 - val_acc: 0.8389\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 80s 5ms/step - loss: 0.3334 - acc: 0.8558 - val_loss: 0.3339 - val_acc: 0.8593\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 80s 5ms/step - loss: 0.2597 - acc: 0.8945 - val_loss: 0.3243 - val_acc: 0.8613\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 78s 4ms/step - loss: 0.2009 - acc: 0.9221 - val_loss: 0.3539 - val_acc: 0.8624\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 76s 4ms/step - loss: 0.1550 - acc: 0.9431 - val_loss: 0.3806 - val_acc: 0.8567\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model3: RNNs and CNNs for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,062,933\n",
      "Trainable params: 62,933\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.layers[0].set_weights([embedding_matrix])\n",
    "model3.layers[0].trainable = False\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 42s 2ms/step - loss: 0.5729 - acc: 0.6951 - val_loss: 0.5180 - val_acc: 0.7444\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 40s 2ms/step - loss: 0.4583 - acc: 0.7887 - val_loss: 0.4297 - val_acc: 0.8045\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 40s 2ms/step - loss: 0.4277 - acc: 0.8057 - val_loss: 0.4352 - val_acc: 0.7913\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 39s 2ms/step - loss: 0.4022 - acc: 0.8194 - val_loss: 0.4155 - val_acc: 0.8103\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 40s 2ms/step - loss: 0.3811 - acc: 0.8308 - val_loss: 0.3823 - val_acc: 0.8260\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model3.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
