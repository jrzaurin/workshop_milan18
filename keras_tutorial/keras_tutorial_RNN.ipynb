{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = 'datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  777   16   28    4    1  115 2278 6887   11   19\n",
      " 1025    5   27    5   42 2425 1861  128 2270    5    3 6985  308    7\n",
      "    7 3383 2373    1   19   36  463 3169    2  222    3 1016  174   20\n",
      "   49  808]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# maximum review length\n",
    "maxlen = 100\n",
    "validation_samples = 10000 \n",
    "# maximum words in our vocabulary\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is a labor of love for paul reiser who understands what it's like to be both a father and a son as well as to have both laughter and tears as you move through life the most fun part though was watching reiser watch falk you could tell it was both his character coming to a new appreciation of his father and a fellow actor really enjoying peter special craft really delightful let's hope this film makes it into theaters around the country sometime soon so everyone can have a chance to laugh and cry with paul reiser and folks\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for el in data[0]:\n",
    "    w = [k for k,v in word_index.items() if v==el]\n",
    "    if w:\n",
    "        words.append(w[0])\n",
    "\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'datasets/glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Embedding Matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1: non trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=maxlen,\n",
    "                    trainable=False))\n",
    "model.add(LSTM(embedding_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# or \n",
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 67s 4ms/step - loss: 0.6100 - acc: 0.6574 - val_loss: 0.4885 - val_acc: 0.7680\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 67s 4ms/step - loss: 0.4781 - acc: 0.7719 - val_loss: 0.4185 - val_acc: 0.8057\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 66s 4ms/step - loss: 0.4273 - acc: 0.8042 - val_loss: 0.4019 - val_acc: 0.8163\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 66s 4ms/step - loss: 0.3960 - acc: 0.8198 - val_loss: 0.3797 - val_acc: 0.8273\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 66s 4ms/step - loss: 0.3787 - acc: 0.8263 - val_loss: 0.3653 - val_acc: 0.8372\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2: trainable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words,\n",
    "                    embedding_dim,\n",
    "                    input_length=maxlen))\n",
    "model2.add(LSTM(embedding_dim, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 81s 5ms/step - loss: 0.5431 - acc: 0.7134 - val_loss: 0.3709 - val_acc: 0.8389\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 80s 5ms/step - loss: 0.3334 - acc: 0.8558 - val_loss: 0.3339 - val_acc: 0.8593\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 80s 5ms/step - loss: 0.2597 - acc: 0.8945 - val_loss: 0.3243 - val_acc: 0.8613\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 78s 4ms/step - loss: 0.2009 - acc: 0.9221 - val_loss: 0.3539 - val_acc: 0.8624\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 76s 4ms/step - loss: 0.1550 - acc: 0.9431 - val_loss: 0.3806 - val_acc: 0.8567\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model3: RNNs and CNNs for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,062,933\n",
      "Trainable params: 62,933\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.layers[0].set_weights([embedding_matrix])\n",
    "model3.layers[0].trainable = False\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/5\n",
      "17500/17500 [==============================] - 42s 2ms/step - loss: 0.5729 - acc: 0.6951 - val_loss: 0.5180 - val_acc: 0.7444\n",
      "Epoch 2/5\n",
      "17500/17500 [==============================] - 40s 2ms/step - loss: 0.4583 - acc: 0.7887 - val_loss: 0.4297 - val_acc: 0.8045\n",
      "Epoch 3/5\n",
      "17500/17500 [==============================] - 40s 2ms/step - loss: 0.4277 - acc: 0.8057 - val_loss: 0.4352 - val_acc: 0.7913\n",
      "Epoch 4/5\n",
      "17500/17500 [==============================] - 39s 2ms/step - loss: 0.4022 - acc: 0.8194 - val_loss: 0.4155 - val_acc: 0.8103\n",
      "Epoch 5/5\n",
      "17500/17500 [==============================] - 40s 2ms/step - loss: 0.3811 - acc: 0.8308 - val_loss: 0.3823 - val_acc: 0.8260\n"
     ]
    }
   ],
   "source": [
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model3.fit(x_train, y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
